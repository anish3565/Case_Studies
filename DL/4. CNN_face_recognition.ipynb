{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765c33ca",
   "metadata": {},
   "source": [
    "## **Face Recognition using Deep Learning CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664222f0",
   "metadata": {},
   "source": [
    "<html><img src=\"https://thinkingneuron.com/wp-content/uploads/2020/10/CNN-case-study.png\" alt=\"Convolution step in CNN\"></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b2a6fd",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network (CNN)**\n",
    "- Focuses on one portion of the image at a time and scanning the whole image.\n",
    "- It boils down every image as a vector of numbers, which can be learned by the fully connected Dense Layers of ANN.\n",
    "\n",
    "<html><img src=\"https://thinkingneuron.com/wp-content/uploads/2020/10/CNN-face-recognition-case-study.png\"></html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a3c061",
   "metadata": {},
   "source": [
    "**Dataset:** \n",
    "- Containes cropped face images of 16 people divided into Training and Testing,\n",
    "- Will,\n",
    "    - Train the CNN model using the images in the **Training folder**\n",
    "    - Test the model using the unseen images from the **Testing folder**, to check if the model is able to recognise the face number of the unseen images or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3931a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deep Learning CNN model to recognize face\n",
    "'''####### IMAGE PRE-PROCESSING for TRAINING and TESTING data #######'''\n",
    "\n",
    "# Specifying the folder where images are present\n",
    "TrainingImagePath='../Datasets/CNN/Face Images/Final Training Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7aee13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f702370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 images belonging to 16 classes.\n",
      "Found 244 images belonging to 16 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'face1': 0,\n",
       " 'face10': 1,\n",
       " 'face11': 2,\n",
       " 'face12': 3,\n",
       " 'face13': 4,\n",
       " 'face14': 5,\n",
       " 'face15': 6,\n",
       " 'face16': 7,\n",
       " 'face2': 8,\n",
       " 'face3': 9,\n",
       " 'face4': 10,\n",
       " 'face5': 11,\n",
       " 'face6': 12,\n",
       " 'face7': 13,\n",
       " 'face8': 14,\n",
       " 'face9': 15}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Defining pre-processing transformations on raw images of training data\n",
    "# These hyper parameters helps to generate slightly twisted versions\n",
    "# of the original images, which leads to a better model, since it learns\n",
    "# on the bad and good mix of images\n",
    "\n",
    "train_datagen=ImageDataGenerator(\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Defining pre-processing transformations on raw images of testing data\n",
    "# No transformations are done on the testing images\n",
    "test_datagen=ImageDataGenerator()\n",
    "\n",
    "# Generating the Training Data\n",
    "training_set=train_datagen.flow_from_directory(\n",
    "    TrainingImagePath,\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Generating the Testing Data\n",
    "test_set=test_datagen.flow_from_directory(\n",
    "    TrainingImagePath,\n",
    "    target_size=(64,64),\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Printing class labels for each class\n",
    "test_set.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b943933",
   "metadata": {},
   "source": [
    "## Creating a mapping for index and a face names\n",
    "\n",
    "- The above class_index dictionary has face names as keys and the numeric mapping as values.\n",
    "- Swapping it, as classifier model will return the answer as the numeric mapping, while answer needs to get the face-name out of it.\n",
    "\n",
    "**As its a multiclass problem,**\n",
    "- Counting the number of unique faces, as it'll be used as the number of output layer of fully connected ANN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "097ece24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lookup table for all faces\n",
    "# Class_indices have the numeric tag for each face\n",
    "TrainClasses=training_set.class_indices\n",
    "\n",
    "# Storing the face and numeric tag\n",
    "ResultMap={}\n",
    "\n",
    "for faceValue, faceName in zip(TrainClasses.values(), TrainClasses.keys()):\n",
    "    ResultMap[faceValue]=faceName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "590a0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing face maps for future reference\n",
    "import pickle\n",
    "with open(\"ResultMap.pkl\", 'wb') as fileWriteStream:\n",
    "    pickle.dump(ResultMap, fileWriteStream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484b869c",
   "metadata": {},
   "source": [
    "#### This model will give answer as a numeric tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81e60a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its ID {0: 'face1', 1: 'face10', 2: 'face11', 3: 'face12', 4: 'face13', 5: 'face14', 6: 'face15', 7: 'face16', 8: 'face2', 9: 'face3', 10: 'face4', 11: 'face5', 12: 'face6', 13: 'face7', 14: 'face8', 15: 'face9'}\n",
      "\n",
      " The Number of output neurons:  16\n"
     ]
    }
   ],
   "source": [
    "# This mapping will help to get the corresponding face name for it\n",
    "print(\"Mapping of Face and its ID\", ResultMap)\n",
    "\n",
    "# The number of neurons for the output layer is equal to the number of faces\n",
    "OutputNeurons=len(ResultMap)\n",
    "print('\\n The Number of output neurons: ', OutputNeurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c5481",
   "metadata": {},
   "source": [
    "## **Creating the CNN face recognition model**\n",
    "\n",
    "Model has:\n",
    "- 2 hidden layers of convolution\n",
    "- 2 hidden layers of max pooling\n",
    "- 1 layer of flattening\n",
    "- 1 output layer with 16 neurons (one for each face)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0023950",
   "metadata": {},
   "source": [
    "Important hyperparameters:\n",
    "\n",
    "- **Filters**=32: \n",
    "    - This number indicated how many filters we are using to look at the image pixels during the convolution step.\n",
    "    - Some filters may catch sharp edges, some filters may catch color variants, some filters may catch outliers, etc.\n",
    "    - In the first layer, the number of filter=32 is commonly used, the increasing the power of 2, i.e, next layer is 64, next is 128 and so on.\n",
    "\n",
    "- **kernel_size**=(5,5):\n",
    "    - Indicated the size of sliding window during convolution, using 5*5 pixels here\n",
    "\n",
    "- **strides**=(1,1):\n",
    "    - How fast/slow should the sliding window move during convolution.\n",
    "    - Using 1*1 (lowest setting)\n",
    "    - Meaning, sliding the convolution window of 5*5 (kernel_size) by 1 pixel in the x-axis and 1 pixel in the y-axis until the whole thing is scanned.\n",
    "\n",
    "- **input_shape**=(64,64,3):\n",
    "    - Images in the dataset were compressed into 64*64 during pre-processing\n",
    "    - Images are matrix of RGB (3 colors) codes.\n",
    "    - This expected shape is (64,64,3) as in 3 arrays of (64,64), one for RGB colors each\n",
    "\n",
    "- **activation**='relu':\n",
    "    - This specifies the activation function for each calculations inside each neuron.\n",
    "    - Thus choosing values like 'relu', 'tanh', 'sigmoid', etc.\n",
    "\n",
    "- **optimizer**='adam':\n",
    "    - This parameter helps to find the optimum values for each weight in the neural network.\n",
    "    - 'adam' is one of the most useful optimizers, another one is 'misprop'\n",
    "\n",
    "- **steps_per_epochs**=8:\n",
    "    - Specifies how many rows will be passed to the Network, in one go after which the error calculation will begin and the neural network will start adjusting its weights based on the errors.\n",
    "    - When all the rows are passed in the batches of 8 rows each as specified in this parameter, then we call that 1-epoch, or one full data cycle.\n",
    "- **Epochs**=60:\n",
    "    - The same activity of adjusting weights continues for 60 times, as specified by this parameter.\n",
    "    - In simple terms, the CNN looks at the full training data 60 times and adjusts its weights, thus tuned based on accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a485dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating CNN deep learning model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "'''Initializing the Convolutional Neural Network'''\n",
    "classifier=Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d9426",
   "metadata": {},
   "source": [
    "### Step 1: **Convolution**\n",
    "- Adding the first layer of CNN\n",
    "- Using the format (64,64,3) as using TensorFlow backend\n",
    "- It means 3 matrix of size (64*64) pixels representing Red, Green and Blue components of pixels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f289e300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Convolution2D(32, kernel_size=(5,5), strides=(1,1), input_shape=(64,64,3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac532fc",
   "metadata": {},
   "source": [
    "### Step 2: **Max Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "407a6ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPool2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3ac72",
   "metadata": {},
   "source": [
    "Adding additional layer of Convolution for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "898daee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Convolution2D(64, kernel_size=(5,5), strides=(1,1), activation='relu'))\n",
    "\n",
    "classifier.add(MaxPool2D(pool_size=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e6f91",
   "metadata": {},
   "source": [
    "### Step 3: **Flattening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bee29376",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3766fd48",
   "metadata": {},
   "source": [
    "### Step 4: **Fully connected Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "504ff276",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(OutputNeurons, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c39037",
   "metadata": {},
   "source": [
    "### **Compiling the CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef7ee6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b97a5",
   "metadata": {},
   "source": [
    "#### Measuring time taken by model to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b66c98d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 1.0000 - val_loss: 1.7192e-04\n",
      "Epoch 2/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 1.5536e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.9952 - loss: 0.0071\n",
      "Epoch 4/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 4.8796e-04\n",
      "Epoch 5/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.9897 - loss: 0.0108 - val_accuracy: 0.9828 - val_loss: 0.0305\n",
      "Epoch 6/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0045 - val_accuracy: 1.0000 - val_loss: 0.0014\n",
      "Epoch 8/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0015\n",
      "Epoch 9/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 1.0000 - loss: 0.0089\n",
      "Epoch 10/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 1.0637e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 0.0024\n",
      "Epoch 12/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 13/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 9.9145e-04 - val_accuracy: 1.0000 - val_loss: 5.8913e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 6.9574e-05\n",
      "Epoch 15/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.9962 - loss: 0.0050 \n",
      "Epoch 16/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 1.9845e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 0.9972 - loss: 0.0056 - val_accuracy: 0.9914 - val_loss: 0.0232\n",
      "Epoch 18/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 19/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.9991 - loss: 0.0126 - val_accuracy: 1.0000 - val_loss: 7.0296e-04\n",
      "Epoch 20/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 21/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.9986 - loss: 0.0059  \n",
      "Epoch 22/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 5.4527e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 24/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 25/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.9872 - loss: 0.0143 - val_accuracy: 1.0000 - val_loss: 6.8415e-05\n",
      "Epoch 26/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 3.5997e-05\n",
      "Epoch 27/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.0017  \n",
      "Epoch 28/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 2.0969e-04\n",
      "Epoch 29/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 7.7459e-04 - val_accuracy: 1.0000 - val_loss: 1.4320e-04\n",
      "Epoch 30/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 31/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 1.0000 - val_loss: 1.5349e-04\n",
      "Epoch 32/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 1.2488e-04\n",
      "Epoch 33/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 1.5699e-04\n",
      "Epoch 34/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 1.7564e-05\n",
      "Epoch 35/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 3.3818e-04 - val_accuracy: 1.0000 - val_loss: 5.7991e-05\n",
      "Epoch 36/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 37/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 1.2545e-04 - val_accuracy: 1.0000 - val_loss: 1.5721e-05\n",
      "Epoch 38/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 2.0041e-05\n",
      "Epoch 39/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 2.4453e-04\n",
      "Epoch 40/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 1.0248e-05\n",
      "Epoch 41/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 1.2226e-04 - val_accuracy: 1.0000 - val_loss: 3.4035e-06\n",
      "Epoch 42/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 43/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 3.3968e-04 - val_accuracy: 1.0000 - val_loss: 5.0670e-06\n",
      "Epoch 44/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 1.1904e-05\n",
      "Epoch 45/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 1.0000 - loss: 7.6389e-05\n",
      "Epoch 46/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 8.5780e-05\n",
      "Epoch 47/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 9.8478e-05 - val_accuracy: 1.0000 - val_loss: 4.2205e-05\n",
      "Epoch 48/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 49/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 3.0910e-04 - val_accuracy: 1.0000 - val_loss: 1.6625e-05\n",
      "Epoch 50/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 1.7710e-05\n",
      "Epoch 51/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 1.0000 - loss: 5.4413e-04\n",
      "Epoch 52/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 3.5801e-05\n",
      "Epoch 53/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 5.7461e-04 - val_accuracy: 1.0000 - val_loss: 2.3211e-05\n",
      "Epoch 54/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 55/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 3.0318e-04 - val_accuracy: 1.0000 - val_loss: 3.5323e-06\n",
      "Epoch 56/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 9.8033e-06\n",
      "Epoch 57/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 1.0000 - loss: 1.9561e-04\n",
      "Epoch 58/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 1.4614e-05\n",
      "Epoch 59/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 9.3775e-04 - val_accuracy: 1.0000 - val_loss: 1.5192e-05\n",
      "Epoch 60/60\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Total Time Taken:  1 Minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "startTime=time.time()\n",
    "\n",
    "# Starting the model training\n",
    "classifier.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=8,\n",
    "    epochs=60,\n",
    "    validation_data=test_set,\n",
    "    validation_steps=4)\n",
    "\n",
    "endTime=time.time()\n",
    "\n",
    "print(\"Total Time Taken: \", round((endTime-startTime)/60), \"Minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2671b92f",
   "metadata": {},
   "source": [
    "## **Testing CNN on unseen images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "186b37e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  face4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "ImagePath='../Datasets/CNN/Face Images/Final Testing Images/face4/3face4.jpg'\n",
    "\n",
    "test_image=image.load_img(ImagePath, target_size=(64,64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "\n",
    "test_image=np.expand_dims(test_image, axis=0)\n",
    "\n",
    "result=classifier.predict(test_image, verbose=0)\n",
    "\n",
    "print('Prediction is: ', ResultMap[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a98748",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Case Study of CNN for face recognition completed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
